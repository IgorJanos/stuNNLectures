{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "31d56fa8489417c6958bc95e73b8d3766816132fea40e62e4bcabc0084854ff7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DropOut\n",
    "\n",
    "Podme implementovat DropOut. Pouzijeme metodu \"Inverted dropout\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from backstage.layer import Layer\n"
   ]
  },
  {
   "source": [
    "## 1. Konstruktor\n",
    "\n",
    "Parametrom vrstvy DropOut je __dropRate__ - sanca, ze neuron bude deaktivovany. Cize __dropRate = 0__ znamena pouzitie vsetkych neuronov, a __dropRate = 0.8__ znamena, ze kazdy neuron ma 80% sancu ze bude vypnuty - pouzijeme iba 20% neuronov."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropOut(Layer):\n",
    "    def __init__(self, dropRate=0.5):\n",
    "        super().__ini__(act='linear')\n",
    "\n",
    "        # Inverted dropout znamena, ze pracujeme s \n",
    "        #    keepProb - pravdepodobnostou ponechania neuronov\n",
    "        self.keepProb = 1.0 - dropRate"
   ]
  },
  {
   "source": [
    "## 2. Inicializacia\n",
    "\n",
    "Dropout vrstva ma rovnaky tvar ako predcahdzajuca vrstva."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropOut(DropOut):\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "\n",
    "        # Pouzivame tvar predchadzajucej vrstvy\n",
    "        self.shape = prevLayer.shape"
   ]
  },
  {
   "source": [
    "## 3. Priamy prechod\n",
    "\n",
    "DropOut je prvy typ vrstvy, ktory sme stretli, ktory ma rozdielne spravanie pocas ucenia a pocas predikcie. Preto rozsirujeme metodu __forward__ o argument __isTraining__.\n",
    "\n",
    "Vypocitame si __mask__ - boolean masku, ktora bude mat hodnoty True alebo False podla toho, ci si dany neuron chceme ponechat, alebo ho deaktivovat. Metoda Inverted Dropout pouziva este korekciu skalovania, ktora zaruci, ze celkovy objem aktivacie zostane zachovany."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropOut(DropOut):\n",
    "\n",
    "    # Novy argument - isTraining\n",
    "    def forward(self, x, isTraining):\n",
    "\n",
    "        if (isTraining):\n",
    "            # Drop masku vzdy pri kazdom prechode a aplikujeme korekciu skalovania\n",
    "            mask = (np.random.rand(*x.shape) < self.keepProb)\n",
    "            mask = dropMask / self.keepProb\n",
    "\n",
    "            # Masku budeme potrebovat pri spatnom prechode\n",
    "            return x*mask, mask\n",
    "\n",
    "        # Pocas predikcie vratime priamo x\n",
    "        return x, None\n"
   ]
  },
  {
   "source": [
    "## 4. Spatny prechod\n",
    "\n",
    "Pouzijeme masku, ktoru sme vratili ako nacachovanu hodnotu. A gradient aktivacie sirime iba cez tie neurony, ktore v priamom prechode zostali aktivne.\n",
    "\n",
    "DropOut nema ziadne parametre, ktore by sa mohli ucit, preto s optimizerom nerobime nic.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropOut(DropOut):\n",
    "\n",
    "    def backward(self, da, aPrev, cache, optimizer):\n",
    "\n",
    "        # Masku sme nacachovali pri priamom prechode\n",
    "        mask = cache\n",
    "\n",
    "        # Gradient sirime iba pre aktivne neurony z priameho prechodu\n",
    "        daPrev = da * mask\n",
    "        return daPrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}