{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "31d56fa8489417c6958bc95e73b8d3766816132fea40e62e4bcabc0084854ff7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Inicializacia\n",
    "\n",
    "Podme si vyskusat, aky bude mat dopad pouzitie vhodnej inicializacie na nase ukazkove modely."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from backstage.utils import dataset_Circles, dataset_Flower, draw_DecisionBoundary, draw_TrainingResults\n",
    "from backstage.layer import Input, Dense, DropOut\n",
    "from backstage.loss import CreateLossFunction\n",
    "from backstage.optimizer import GradientDescent\n",
    "from backstage.model import Model"
   ]
  },
  {
   "source": [
    "## 1. Implementacia inicializatora\n",
    "\n",
    "Zadeklarujme si BaseClass pre inicializator."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, nin, nout):\n",
    "        pass"
   ]
  },
  {
   "source": [
    "Skusme zakladne inicializatory pre Uniform a Normal rozdelenia."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(Initializer):\n",
    "    def __init__(self, mean=0.0, stddev=1.0):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def __call__(self, nin, nout):\n",
    "        # Spravime nahodny tensor s rozmermi (Nout x Nin)\n",
    "        t = np.random.randn(nout, nin)\n",
    "\n",
    "        # Posunieme Mean a deviation\n",
    "        t = (t * self.stddev) + t.mean\n",
    "        return t"
   ]
  },
  {
   "source": [
    "A skusme teraz pouzit inicializator v nasom frameworku"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Dense):\n",
    "    def __init__(self, nunits, act='linear', weightsInitializer=Normal):\n",
    "        super().__init__(act)        \n",
    "        self.nunits = nunits                            # Pocet neuronov vrstvy\n",
    "        self.W = None                                   # Maticu vah zatial nemame\n",
    "        self.b = None                                   # Bias zatial nemame\n",
    "        self.optimizerContext = None                    # Kontext zatial nemame\n",
    "        self.weightsInitializer = weightsInitializer    # Odlozime inicializer\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        # 1. Inicializacia\n",
    "        self.shape = (self.nunits, 1)\n",
    "        self.optimizerContext = None\n",
    "\n",
    "        # Potom ziskame shape z predchadzajucej vrstvy, aby sme vedeli \n",
    "        # urcit rozmer pre maticu vah\n",
    "        pnx, _ = prevLayer.shape\n",
    "        nx, _ = self.shape\n",
    "\n",
    "        # Inicializujeme vahy a bias. Pouzijeme inicializator vah z konstruktoru\n",
    "        self.W = self.weightsInitializer(nin=pnx, nout=nx)\n",
    "        self.b = np.zeros((nx, 1), dtype=float)"
   ]
  },
  {
   "source": [
    "## 2. Xavier Initializer\n",
    "\n",
    "Teraz zimplementujeme Xavier (Glorot) Initializer podla slajdu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierNormal(Initializer):\n",
    "    def __super__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, nin, nout):\n",
    "\n",
    "        # Spocitame pozadovanu varianciu\n",
    "        variance = 2.0 / (nin + nout)\n",
    "\n",
    "        # Spravime nahodny tensor s rozmermi (Nout x Nin) a prenasobime scale factorom\n",
    "        t = np.random.randn(nout, nin) * np.sqrt(variance)\n",
    "        return t"
   ]
  },
  {
   "source": [
    "## 3. He Initializer\n",
    "\n",
    "A pre uplnost este aj He Initializer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeNormal(Initializer):\n",
    "    def __super__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, nin, nout):\n",
    "\n",
    "        # Spocitame pozadovanu varianciu\n",
    "        variance = 4.0 / (nin + nout)\n",
    "\n",
    "        # Spravime nahodny tensor s rozmermi (Nout x Nin) a prenasobime scale factorom\n",
    "        t = np.random.randn(nout, nin) * np.sqrt(variance)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}