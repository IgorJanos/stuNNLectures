{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 4. Vrstvy\n",
    "\n",
    "Teraz si naimplementujeme koncept vrstiev. Zatial spravime 2 vrstvy:\n",
    "- Input\n",
    "- Dense\n",
    "\n",
    "Vrstva predstavuje jednu funkciu vypoctoveho grafu. Tiez podporuje priamy a spatny prechod. Ma navyse informaciu o svojich rozmeroch (pocet neuronov) a dokaze spolupracovat s Optimizerom.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from activation import CreateActivationFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, act='linear'):\n",
    "        # Default shape\n",
    "        self.shape = (0, 0)\n",
    "        # Kazda vrstva by mala mat svoju aktivacnu funkciu. Vieme ju vytvarat podla mena.\n",
    "        self.activation = CreateActivationFunction(act)\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        # 1. Inicializacia parametrov - vahy, bias\n",
    "        #    Mame pristup k predchadzajucej vrstve, aby sme vedeli zistit jej shape\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2. Priamy prechod\n",
    "        #    Na vstupe mame aktivaciu predchadzajucej vrstvy\n",
    "        #    Vystupom je aktivacia a cache\n",
    "        pass\n",
    "\n",
    "    def backward(self, da, aPrev, cache, optimizer):\n",
    "        # 3. Spatny prechod\n",
    "        #    Na vstupe mame:\n",
    "        #        da        = dL/da aktualnej vrstvy\n",
    "        #        aPrev     = aktivacia predchadzajucej vrstvy\n",
    "        #        cache     = medzivysledky z priameho prechodu\n",
    "        #        optimizer = instancia optimizera, s ktorym spolupracujeme\n",
    "        pass\n",
    "\n",
    "    def update(self, optimizer):\n",
    "        # 4. Zostup podla gradientu\n",
    "        #    Na vstupe mame:\n",
    "        #        optimizer = instancia optimizera, s ktorym zostup vykonavame\n",
    "        pass"
   ]
  },
  {
   "source": [
    "### Input \n",
    "\n",
    "Podme teraz naimplementovat vstupnu vrstvu. Pre nu je specificke, ze pocet vstupnych featurov NX dostane v konstruktore. Priamo na vystup dava, co dostane (A0=X). Nevykonava ziadny spatny vypocet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    def __init__(self, nx):\n",
    "        super().__init__(act='linear')\n",
    "\n",
    "        # Vstupny tvar\n",
    "        self.nx = nx\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        self.shape = (self.nx, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x, None\n",
    "\n",
    "    def backward(self, da, aPrev, cache, optimizer):\n",
    "        return None, None\n",
    "    "
   ]
  },
  {
   "source": [
    "### Dense\n",
    "\n",
    "Podme naimplementovat Dense vrstvu - vsetky neurony vrstvy su spojene so vsetkymi neuronmi predchadzajucej vrstvy. Vola sa tiez niekedy Fully Connected layer.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, nunits, act='linear'):\n",
    "        super().__init__(act)        \n",
    "        self.nunits = nunits             # Pocet neuronov vrstvy\n",
    "        self.W = None                    # Maticu vah zatial nemame\n",
    "        self.b = None                    # Bias zatial nemame\n",
    "        self.optimizerContext = None     # Kontext zatial nemame\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        # 1. Inicializacia\n",
    "\n",
    "        # Najskor spocitame nas shape\n",
    "        self.shape = (self.nunits, 1)\n",
    "        self.optimizerContext = None\n",
    "\n",
    "        # Potom ziskame shape z predchadzajucej vrstvy, aby sme vedeli \n",
    "        # urcit rozmer pre maticu vah\n",
    "        pnx, _ = prevLayer.shape\n",
    "        nx, _ = self.shape\n",
    "\n",
    "        # Inicializujeme vahy a bias\n",
    "        self.W = np.random.randn(nx, pnx)\n",
    "        self.b = np.zeros((nx, 1), dtype=float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2. Priamy prechod\n",
    "        z = np.matmul(self.W, x) + self.b     # z = Wx + b\n",
    "        a = self.activation(z)                # a = activation(z)\n",
    "\n",
    "        # Vratime aktivaciu a cache\n",
    "        return a, (self.W, self.b, z)\n",
    "\n",
    "    def backward(self, da, aPrev, cache, optimizer):\n",
    "        # 3. Spatny prechod\n",
    "\n",
    "        # Rozbalime medzivysledky z cache\n",
    "        W, b, z = cache\n",
    "\n",
    "        # Vektorizacia - potrebujeme vediet pocet m\n",
    "        _, m = da.shape\n",
    "\n",
    "        # Spocitame spatny prechod\n",
    "        dz = np.multiply(da, self.activation.derivative(z))\n",
    "        dW = (1.0/m) * np.matmul(dz, aPrev.T)\n",
    "        db = (1.0/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "\n",
    "        # Vysledok - da pre predchadzajucu vrstvu \n",
    "        daPrev = np.matmul(W.T, dz)\n",
    "\n",
    "        # Spolupraca s optimizerom - ukazeme mu gradienty dW, db\n",
    "        # a dostaneme novy kontext\n",
    "        self.optimizerContext = optimizer.backward(self.optimizerContext, dW, db)\n",
    "        return daPrev\n",
    "\n",
    "    def update(self, optimizer):\n",
    "        # 4. Uprava parametrov - spolupraca s optimizerom\n",
    "        self.W, self.b = optimizer.update(self.optimizerContext, self.W, self.b)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}